# =============================================================================
# Grafana - Observability Dashboard & Alerting
# =============================================================================
#
# Grafana is the visualization layer that ties everything together:
#   - Prometheus → metrics dashboards (JVM, HTTP, Kafka, circuit breakers)
#   - Jaeger → trace exploration (click a trace to see the full request flow)
#   - Loki → log search (filter by service, level, correlation ID)
#   - Unified Alerting → Teams & Email notifications
#
# We pre-provision:
#   1. Datasources: Prometheus, Jaeger, Loki (auto-configured on startup)
#   2. Dashboards: Service overview, failed events monitoring
#   3. Alerting: Contact points (Teams, Email), notification policies, alert rules
#
# Credentials:
#   Teams webhook URL and SMTP settings are stored in Vault at
#   secret/grafana/alerting. A Vault Agent init container fetches them
#   at pod startup and writes them to /vault/secrets/ (in-memory volume).
#
#   To update credentials:
#     export VAULT_ADDR=http://localhost:8200  # after port-forward
#     vault kv put secret/grafana/alerting \
#       teams-webhook-url='https://YOUR_ORG.webhook.office.com/...' \
#       smtp-host='smtp.gmail.com:587' \
#       smtp-user='alerts@yourcompany.com' \
#       smtp-password='your-app-password' \
#       smtp-from-address='alerts@yourcompany.com' \
#       alert-email-to='oncall-team@yourcompany.com'
#     Then restart Grafana: kubectl rollout restart deployment/grafana -n retail-observe
#
# Access: http://localhost:30030 (admin/admin)
# =============================================================================

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: retail-observe
data:
  datasources.yaml: |
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        access: proxy
        url: http://prometheus.retail-observe.svc.cluster.local:9090
        isDefault: true
        editable: false

      - name: Jaeger
        type: jaeger
        access: proxy
        url: http://jaeger.retail-observe.svc.cluster.local:16686
        editable: false

      - name: Loki
        type: loki
        access: proxy
        url: http://loki.retail-observe.svc.cluster.local:3100
        editable: false

---
# Alerting contact points — Teams webhook + Email
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-alerting-contactpoints
  namespace: retail-observe
data:
  contactpoints.yaml: |
    apiVersion: 1
    contactPoints:
      - orgId: 1
        name: microsoft-teams
        receivers:
          - uid: teams-receiver
            type: teams
            settings:
              url: ${TEAMS_WEBHOOK_URL}
              title: '[{{ .Status | toUpper }}:{{ .Alerts.Firing | len }}] Retail Platform Alert'
              sectiontitle: ''
              message: |-
                **Alerts Firing:**
                {{ range .Alerts.Firing }}
                - **{{ .Labels.alertname }}** ({{ .Labels.severity }})
                  {{ .Annotations.summary }}
                  {{ .Annotations.description }}
                {{ end }}
                {{ if .Alerts.Resolved }}
                **Resolved:**
                {{ range .Alerts.Resolved }}
                - **{{ .Labels.alertname }}**: {{ .Annotations.summary }}
                {{ end }}
                {{ end }}
            disableResolveMessage: false
      - orgId: 1
        name: email
        receivers:
          - uid: email-receiver
            type: email
            settings:
              addresses: ${ALERT_EMAIL_TO}
              singleEmail: false
            disableResolveMessage: false

---
# Mute timing — "Deployment Window" (always deployed, toggled by deploy scripts)
# Default schedule targets year 2000 so it never matches. The deploy script
# activates it by updating the schedule to cover the current time via API,
# then resets it back when the rollout finishes.
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-alerting-mutetimings
  namespace: retail-observe
data:
  mutetimings.yaml: |
    apiVersion: 1
    muteTimes:
      - orgId: 1
        name: deployment-window
        time_intervals:
          - times:
              - start_time: "00:00"
                end_time: "00:01"
            years:
              - "2000"

---
# Notification policies — route alerts by severity
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-alerting-policies
  namespace: retail-observe
data:
  policies.yaml: |
    apiVersion: 1
    policies:
      - orgId: 1
        receiver: microsoft-teams
        group_by:
          - grafana_folder
          - alertname
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        routes:
          - receiver: email
            matchers:
              - severity = critical
            group_wait: 30s
            group_interval: 5m
            repeat_interval: 4h
            mute_time_intervals:
              - deployment-window
            continue: true
          - receiver: microsoft-teams
            matchers:
              - severity = critical
            group_wait: 30s
            group_interval: 5m
            repeat_interval: 4h
            mute_time_intervals:
              - deployment-window
            continue: false
          - receiver: microsoft-teams
            matchers:
              - severity = warning
            group_wait: 1m
            group_interval: 10m
            repeat_interval: 12h
            mute_time_intervals:
              - deployment-window
            continue: false

---
# Alert rules — Prometheus-based health, performance, and business alerts
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-alerting-rules
  namespace: retail-observe
data:
  rules.yaml: |
    apiVersion: 1
    groups:
      - orgId: 1
        name: Retail Platform Alerts
        folder: Retail Platform
        interval: 1m
        rules:

          # ── Availability ──────────────────────────────────────────────
          - uid: alert-pod-crashloop
            title: Pod CrashLooping
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: increase(kube_pod_container_status_restarts_total{namespace=~"retail-app|retail-data"}[10m])
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: max
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [3] }
            noDataState: OK
            for: 5m
            labels:
              severity: critical
              category: availability
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has restarted {{ $values.B }} times in 10m"
              description: "Container {{ $labels.container }} is crash-looping. Check logs and events."

          - uid: alert-pod-not-ready
            title: Pod Not Ready
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: kube_pod_status_ready{namespace=~"retail-app|retail-data", condition="true"} == 0
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: lt, params: [1] }
            noDataState: OK
            for: 5m
            labels:
              severity: warning
              category: availability
            annotations:
              summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"
              description: "Pod has been unready for more than 5 minutes."

          - uid: alert-deployment-mismatch
            title: Deployment Replicas Mismatch
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: kube_deployment_spec_replicas{namespace=~"retail-app"} - kube_deployment_status_replicas_available{namespace=~"retail-app"}
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [0] }
            noDataState: OK
            for: 5m
            labels:
              severity: warning
              category: availability
            annotations:
              summary: "Deployment {{ $labels.deployment }} has unavailable replicas"
              description: "Desired replicas do not match available for 5+ minutes."

          # ── Performance ───────────────────────────────────────────────
          - uid: alert-high-error-rate
            title: High HTTP 5xx Error Rate
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: >
                    sum(rate(http_server_requests_seconds_count{namespace="retail-app", status=~"5.."}[5m])) by (app)
                    / sum(rate(http_server_requests_seconds_count{namespace="retail-app"}[5m])) by (app) * 100
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [5] }
            noDataState: OK
            for: 5m
            labels:
              severity: critical
              category: performance
            annotations:
              summary: "{{ $labels.app }} 5xx error rate is {{ $values.B }}%"
              description: "HTTP 5xx error rate exceeds 5% for 5 minutes."

          - uid: alert-high-latency
            title: High Request Latency (P99 > 2s)
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: >
                    histogram_quantile(0.99,
                      sum(rate(http_server_requests_seconds_bucket{namespace="retail-app"}[5m])) by (le, app)
                    ) * 1000
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [2000] }
            noDataState: OK
            for: 5m
            labels:
              severity: warning
              category: performance
            annotations:
              summary: "{{ $labels.app }} P99 latency is {{ $values.B }}ms"
              description: "Request latency P99 exceeds 2 seconds for 5 minutes."

          # ── Infrastructure ────────────────────────────────────────────
          - uid: alert-node-cpu-high
            title: Node CPU Usage High
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: 100 - (avg by (node) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [85] }
            noDataState: OK
            for: 10m
            labels:
              severity: warning
              category: infrastructure
            annotations:
              summary: "Node {{ $labels.node }} CPU at {{ $values.B }}%"
              description: "Node CPU usage has been above 85% for 10 minutes."

          - uid: alert-node-memory-high
            title: Node Memory Usage High
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [90] }
            noDataState: OK
            for: 10m
            labels:
              severity: critical
              category: infrastructure
            annotations:
              summary: "Node memory at {{ $values.B }}%"
              description: "Node memory usage has been above 90% for 10 minutes."

          - uid: alert-node-disk-high
            title: Node Disk Usage High
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: (1 - node_filesystem_avail_bytes{fstype="ext4"} / node_filesystem_size_bytes{fstype="ext4"}) * 100
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [85] }
            noDataState: OK
            for: 5m
            labels:
              severity: warning
              category: infrastructure
            annotations:
              summary: "Disk usage at {{ $values.B }}% on {{ $labels.instance }}"
              description: "Disk usage exceeds 85%."

          # ── Business ──────────────────────────────────────────────────
          - uid: alert-dlt-events
            title: Dead Letter Topic Events Detected
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: sum(increase(kafka_dlt_events_total{namespace="retail-app"}[5m])) by (app)
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: max
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [0] }
            noDataState: OK
            for: 1m
            labels:
              severity: critical
              category: business
            annotations:
              summary: "DLT events detected in {{ $labels.app }}"
              description: "Messages are landing in dead-letter topics — order processing failures."

          - uid: alert-kafka-consumer-lag
            title: Kafka Consumer Lag High
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: sum(kafka_consumer_fetch_manager_records_lag_max{namespace="retail-app"}) by (app)
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [500] }
            noDataState: OK
            for: 10m
            labels:
              severity: warning
              category: business
            annotations:
              summary: "Kafka consumer lag for {{ $labels.app }} is {{ $values.B }}"
              description: "Consumer lag exceeds 500 for 10 minutes. Messages are piling up."

          - uid: alert-db-connection-pool-exhaustion
            title: Database Connection Pool Exhaustion
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: hikaricp_connections_pending{namespace="retail-app"}
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [0] }
            noDataState: OK
            for: 5m
            labels:
              severity: warning
              category: performance
            annotations:
              summary: "{{ $labels.app }} has {{ $values.B }} pending DB connections"
              description: "Threads are waiting for database connections. Pool may be exhausted."

          - uid: alert-postgres-down
            title: PostgreSQL Down
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: pg_up
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: lt, params: [1] }
            noDataState: Alerting
            for: 1m
            labels:
              severity: critical
              category: infrastructure
            annotations:
              summary: "PostgreSQL is DOWN"
              description: "The pg_up metric indicates PostgreSQL is unreachable."

          - uid: alert-prometheus-down
            title: Prometheus Down
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 300, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: up{job="prometheus"}
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 300, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: lt, params: [1] }
            noDataState: Alerting
            for: 1m
            labels:
              severity: critical
              category: infrastructure
            annotations:
              summary: "Prometheus is DOWN"
              description: "The Prometheus scrape target is unreachable. All monitoring and alerting is degraded."

          # ── Dragonfly Cache ───────────────────────────────────────────
          - uid: alert-dragonfly-down
            title: Dragonfly Down
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: up{job="dragonfly"}
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: lt, params: [1] }
            noDataState: Alerting
            for: 1m
            labels:
              severity: critical
              category: infrastructure
            annotations:
              summary: "Dragonfly cache is DOWN"
              description: "The Dragonfly instance is unreachable. All L2 cache operations will fail."

          - uid: alert-dragonfly-memory-high
            title: Dragonfly Memory Usage High
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: dragonfly_used_memory_bytes{job="dragonfly"} / dragonfly_maxmemory_bytes{job="dragonfly"} * 100
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [80] }
            noDataState: OK
            for: 5m
            labels:
              severity: warning
              category: infrastructure
            annotations:
              summary: "Dragonfly memory at {{ $values.B }}%"
              description: "Dragonfly memory usage exceeds 80% of maxmemory. Evictions may increase."

          - uid: alert-dragonfly-connections-high
            title: Dragonfly Connected Clients High
            condition: C
            data:
              - refId: A
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: PBFA97CFB590B2093
                model:
                  expr: dragonfly_connected_clients{job="dragonfly"}
                  instant: false
                  range: true
              - refId: B
                relativeTimeRange: { from: 600, to: 0 }
                datasourceUid: __expr__
                model:
                  type: reduce
                  expression: A
                  reducer: last
              - refId: C
                datasourceUid: __expr__
                model:
                  type: threshold
                  expression: B
                  conditions:
                    - evaluator: { type: gt, params: [100] }
            noDataState: OK
            for: 5m
            labels:
              severity: warning
              category: infrastructure
            annotations:
              summary: "Dragonfly has {{ $values.B }} connected clients"
              description: "High number of connections to Dragonfly. Check for connection leaks."

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-provider
  namespace: retail-observe
data:
  dashboards.yaml: |
    apiVersion: 1
    providers:
      - name: 'default'
        orgId: 1
        folder: 'Retail Platform'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards
          foldersFromFilesStructure: false

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: retail-observe
data:
  service-overview.json: |
    {
      "id": null, "uid": "retail-overview",
      "title": "Retail Platform - Service Overview",
      "tags": ["retail", "overview"], "timezone": "browser",
      "refresh": "10s", "time": { "from": "now-1h", "to": "now" },
      "templating": { "list": [{ "name": "service", "label": "Service", "type": "custom",
        "query": "All : .+,user-service : user-service,order-service : order-service,payment-service : payment-service,inventory-service : inventory-service",
        "current": { "text": "All", "value": ".+" },
        "options": [
          { "text": "All", "value": ".+", "selected": true },
          { "text": "user-service", "value": "user-service", "selected": false },
          { "text": "order-service", "value": "order-service", "selected": false },
          { "text": "payment-service", "value": "payment-service", "selected": false },
          { "text": "inventory-service", "value": "inventory-service", "selected": false }
        ] }] },
      "panels": [
        { "id": 1, "title": "HTTP Request Rate (per service)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 },
          "targets": [{ "expr": "sum(rate(http_server_requests_seconds_count{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 2, "title": "HTTP Error Rate (5xx)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 },
          "targets": [{ "expr": "sum(rate(http_server_requests_seconds_count{namespace=\"retail-app\", app=~\"${service}\", status=~\"5..\"}[5m])) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 3, "title": "HTTP Avg Latency / Max Latency (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 },
          "targets": [
            { "expr": "sum(rate(http_server_requests_seconds_sum{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) / sum(rate(http_server_requests_seconds_count{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) * 1000", "legendFormat": "avg {{app}}" },
            { "expr": "max(http_server_requests_seconds_max{namespace=\"retail-app\", app=~\"${service}\"}) by (app) * 1000", "legendFormat": "max {{app}}" }
          ] },
        { "id": 4, "title": "HTTP 4xx Client Errors", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 },
          "targets": [{ "expr": "sum(rate(http_server_requests_seconds_count{namespace=\"retail-app\", app=~\"${service}\", status=~\"4..\"}[5m])) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 5, "title": "Application Error Logs (logback)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 },
          "targets": [{ "expr": "sum(rate(logback_events_total{namespace=\"retail-app\", app=~\"${service}\", level=\"error\"}[5m])) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 6, "title": "Circuit Breaker (Envoy) - Open Requests", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 16 },
          "description": "Envoy circuit breaker open state. Non-zero = circuit is open, requests being shed.",
          "targets": [
            { "expr": "sum(envoy_cluster_circuit_breakers_default_rq_open{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "rq open {{app}}" },
            { "expr": "sum(envoy_cluster_circuit_breakers_default_cx_open{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "cx open {{app}}" }
          ] },
        { "id": 7, "title": "Active HTTP Requests (in-flight)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 20 },
          "targets": [{ "expr": "sum(http_server_requests_active_seconds_count{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 8, "title": "Spring Security Auth Latency (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 24 },
          "targets": [{ "expr": "sum(rate(spring_security_http_secured_requests_seconds_sum{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) / sum(rate(spring_security_http_secured_requests_seconds_count{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) * 1000", "legendFormat": "{{app}}" }] },
        { "id": 9, "title": "Caffeine Cache Hit Ratio %", "type": "stat",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 32 },
          "description": "Application-level in-memory cache (Caffeine). High hit ratio = fewer DB queries.",
          "targets": [{ "expr": "sum(cache_gets_total{namespace=\"retail-app\", app=~\"${service}\", result=\"hit\"}) by (app, cache) / sum(cache_gets_total{namespace=\"retail-app\", app=~\"${service}\"}) by (app, cache) * 100", "legendFormat": "{{app}} ({{cache}})" }],
          "options": { "reduceOptions": { "calcs": ["lastNotNull"] }, "textMode": "value_and_name", "graphMode": "none", "orientation": "horizontal" },
          "fieldConfig": { "defaults": { "unit": "percent", "decimals": 1, "min": 0, "max": 100, "thresholds": { "steps": [{"color": "red", "value": null}, {"color": "yellow", "value": 70}, {"color": "green", "value": 90}] } } } },
        { "id": 10, "title": "Caffeine Cache Operations (rate/s)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 32 },
          "description": "Cache get rate split by hit/miss. Sustained misses = cache not effective or TTL too short.",
          "targets": [
            { "expr": "sum(rate(cache_gets_total{namespace=\"retail-app\", app=~\"${service}\", result=\"hit\"}[5m])) by (app, cache)", "legendFormat": "hit {{app}} ({{cache}})" },
            { "expr": "sum(rate(cache_gets_total{namespace=\"retail-app\", app=~\"${service}\", result=\"miss\"}[5m])) by (app, cache)", "legendFormat": "miss {{app}} ({{cache}})" }
          ] },
        { "id": 11, "title": "Caffeine Cache Size & Evictions", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 40 },
          "description": "Current entries vs evictions. High eviction rate = maximumSize too small.",
          "targets": [
            { "expr": "sum(cache_size{namespace=\"retail-app\", app=~\"${service}\"}) by (app, cache)", "legendFormat": "size {{app}} ({{cache}})" },
            { "expr": "sum(rate(cache_evictions_total{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app, cache)", "legendFormat": "evictions/s {{app}} ({{cache}})" }
          ] }
      ]
    }
  jvm-deep-dive.json: |
    {
      "id": null, "uid": "retail-jvm",
      "title": "Retail Platform - JVM Deep Dive",
      "tags": ["retail", "jvm"], "timezone": "browser",
      "refresh": "10s", "time": { "from": "now-1h", "to": "now" },
      "templating": { "list": [{ "name": "service", "label": "Service", "type": "custom",
        "query": "All : .+,user-service : user-service,order-service : order-service,payment-service : payment-service,inventory-service : inventory-service",
        "current": { "text": "All", "value": ".+" },
        "options": [
          { "text": "All", "value": ".+", "selected": true },
          { "text": "user-service", "value": "user-service", "selected": false },
          { "text": "order-service", "value": "order-service", "selected": false },
          { "text": "payment-service", "value": "payment-service", "selected": false },
          { "text": "inventory-service", "value": "inventory-service", "selected": false }
        ] }] },
      "panels": [
        { "id": 1, "title": "Heap Memory Used (MB)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 },
          "targets": [{ "expr": "sum(jvm_memory_used_bytes{namespace=\"retail-app\", app=~\"${service}\", area=\"heap\"}) by (app) / 1048576", "legendFormat": "{{app}}" }] },
        { "id": 2, "title": "Non-Heap Memory (Metaspace) MB", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 },
          "targets": [{ "expr": "sum(jvm_memory_used_bytes{namespace=\"retail-app\", app=~\"${service}\", area=\"nonheap\"}) by (app) / 1048576", "legendFormat": "{{app}}" }] },
        { "id": 3, "title": "GC Pause Duration (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 },
          "targets": [{ "expr": "sum(rate(jvm_gc_pause_seconds_sum{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app, action) * 1000", "legendFormat": "{{app}} {{action}}" }] },
        { "id": 4, "title": "GC Pause Count (rate)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 },
          "targets": [{ "expr": "sum(rate(jvm_gc_pause_seconds_count{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app, action)", "legendFormat": "{{app}} {{action}}" }] },
        { "id": 5, "title": "Memory Allocation Rate (MB/s)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 },
          "targets": [{ "expr": "sum(rate(jvm_gc_memory_allocated_bytes_total{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) / 1048576", "legendFormat": "{{app}}" }] },
        { "id": 6, "title": "GC Overhead %", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 16 },
          "targets": [{ "expr": "sum(jvm_gc_overhead{namespace=\"retail-app\", app=~\"${service}\"}) by (app) * 100", "legendFormat": "{{app}}" }] },
        { "id": 7, "title": "Live Threads", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 24 },
          "targets": [{ "expr": "sum(jvm_threads_live_threads{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 8, "title": "Thread States", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 24 },
          "targets": [{ "expr": "sum(jvm_threads_states_threads{namespace=\"retail-app\", app=~\"${service}\"}) by (app, state)", "legendFormat": "{{app}} {{state}}" }] },
        { "id": 9, "title": "Direct Buffer Memory (MB)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 32 },
          "targets": [{ "expr": "sum(jvm_buffer_memory_used_bytes{namespace=\"retail-app\", app=~\"${service}\"}) by (app, id) / 1048576", "legendFormat": "{{app}} {{id}}" }] },
        { "id": 10, "title": "Loaded Classes", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 32 },
          "targets": [{ "expr": "sum(jvm_classes_loaded_classes{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "{{app}}" }] }
      ]
    }
  database-hikari.json: |
    {
      "id": null, "uid": "retail-db",
      "title": "Retail Platform - Database & HikariCP",
      "tags": ["retail", "database", "hikari"], "timezone": "browser",
      "refresh": "10s", "time": { "from": "now-1h", "to": "now" },
      "templating": { "list": [{ "name": "service", "label": "Service", "type": "custom",
        "query": "All : .+,user-service : user-service,order-service : order-service,payment-service : payment-service,inventory-service : inventory-service",
        "current": { "text": "All", "value": ".+" },
        "options": [
          { "text": "All", "value": ".+", "selected": true },
          { "text": "user-service", "value": "user-service", "selected": false },
          { "text": "order-service", "value": "order-service", "selected": false },
          { "text": "payment-service", "value": "payment-service", "selected": false },
          { "text": "inventory-service", "value": "inventory-service", "selected": false }
        ] }] },
      "panels": [
        { "id": 1, "title": "Active DB Connections", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 },
          "targets": [{ "expr": "sum(hikaricp_connections_active{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 2, "title": "Idle DB Connections", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 },
          "targets": [{ "expr": "sum(hikaricp_connections_idle{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 3, "title": "Pending Connection Requests (waiting threads)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 },
          "description": "Threads waiting for a DB connection. Sustained > 0 = pool exhaustion risk.",
          "targets": [{ "expr": "sum(hikaricp_connections_pending{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 4, "title": "Connection Timeouts (cumulative)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 },
          "description": "Connection acquisition timeouts. Any increase = requests failing due to pool exhaustion.",
          "targets": [{ "expr": "sum(increase(hikaricp_connections_timeout_total{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 5, "title": "Connection Acquire Time (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 },
          "targets": [{ "expr": "sum(rate(hikaricp_connections_acquire_seconds_sum{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) / sum(rate(hikaricp_connections_acquire_seconds_count{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) * 1000", "legendFormat": "{{app}}" }] },
        { "id": 6, "title": "Connection Usage Time (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 16 },
          "targets": [{ "expr": "sum(rate(hikaricp_connections_usage_seconds_sum{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) / sum(rate(hikaricp_connections_usage_seconds_count{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) * 1000", "legendFormat": "{{app}}" }] },
        { "id": 7, "title": "Pool Size vs Max", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 24 },
          "targets": [
            { "expr": "sum(hikaricp_connections{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "current {{app}}" },
            { "expr": "sum(hikaricp_connections_max{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "max {{app}}" }
          ] },
        { "id": 8, "title": "Spring Data Repository Latency (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 24 },
          "targets": [{ "expr": "sum(rate(spring_data_repository_invocations_seconds_sum{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app, repository, method) / sum(rate(spring_data_repository_invocations_seconds_count{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app, repository, method) * 1000", "legendFormat": "{{app}} {{repository}}.{{method}}" }] }
      ]
    }
  kafka-deep-dive.json: |
    {
      "id": null, "uid": "retail-kafka",
      "title": "Retail Platform - Kafka Deep Dive",
      "tags": ["retail", "kafka"], "timezone": "browser",
      "refresh": "10s", "time": { "from": "now-1h", "to": "now" },
      "templating": { "list": [{ "name": "service", "label": "Service", "type": "custom",
        "query": "All : .+,order-service : order-service,payment-service : payment-service,inventory-service : inventory-service",
        "current": { "text": "All", "value": ".+" },
        "options": [
          { "text": "All", "value": ".+", "selected": true },
          { "text": "order-service", "value": "order-service", "selected": false },
          { "text": "payment-service", "value": "payment-service", "selected": false },
          { "text": "inventory-service", "value": "inventory-service", "selected": false }
        ] }] },
      "panels": [
        { "id": 1, "title": "Consumer Lag (max)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 },
          "targets": [{ "expr": "sum(kafka_consumer_fetch_manager_records_lag_max{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 2, "title": "Records Consumed (rate/s)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 },
          "targets": [{ "expr": "sum(rate(kafka_consumer_fetch_manager_records_consumed_total{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 3, "title": "Kafka Listener Processing Time (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 },
          "targets": [{ "expr": "sum(rate(spring_kafka_listener_seconds_sum{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) / sum(rate(spring_kafka_listener_seconds_count{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) * 1000", "legendFormat": "{{app}}" }] },
        { "id": 4, "title": "Kafka Producer Send Time (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 },
          "targets": [{ "expr": "sum(rate(spring_kafka_template_seconds_sum{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) / sum(rate(spring_kafka_template_seconds_count{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app) * 1000", "legendFormat": "{{app}}" }] },
        { "id": 5, "title": "Consumer Rebalances (rate)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 },
          "description": "Frequent rebalances = consumer instability. Check for long processing times or network issues.",
          "targets": [{ "expr": "sum(rate(kafka_consumer_coordinator_rebalance_total{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 6, "title": "Consumer Heartbeat Lag (seconds)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 16 },
          "targets": [{ "expr": "max(kafka_consumer_coordinator_last_heartbeat_seconds_ago{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 7, "title": "DLT Events (rate)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 24 },
          "targets": [{ "expr": "sum(increase(kafka_dlt_events_total{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 8, "title": "Consumer Network I/O (bytes/s)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 24 },
          "targets": [
            { "expr": "sum(rate(kafka_consumer_incoming_byte_total{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app)", "legendFormat": "in {{app}}" },
            { "expr": "sum(rate(kafka_consumer_outgoing_byte_total{namespace=\"retail-app\", app=~\"${service}\"}[5m])) by (app)", "legendFormat": "out {{app}}" }
          ] }
      ]
    }
  system-resources.json: |
    {
      "id": null, "uid": "retail-system",
      "title": "Retail Platform - System Resources",
      "tags": ["retail", "system", "cpu", "memory"], "timezone": "browser",
      "refresh": "10s", "time": { "from": "now-1h", "to": "now" },
      "templating": { "list": [{ "name": "service", "label": "Service", "type": "custom",
        "query": "All : .+,user-service : user-service,order-service : order-service,payment-service : payment-service,inventory-service : inventory-service",
        "current": { "text": "All", "value": ".+" },
        "options": [
          { "text": "All", "value": ".+", "selected": true },
          { "text": "user-service", "value": "user-service", "selected": false },
          { "text": "order-service", "value": "order-service", "selected": false },
          { "text": "payment-service", "value": "payment-service", "selected": false },
          { "text": "inventory-service", "value": "inventory-service", "selected": false }
        ] }] },
      "panels": [
        { "id": 1, "title": "Process CPU Usage %", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 },
          "targets": [{ "expr": "avg(process_cpu_usage{namespace=\"retail-app\", app=~\"${service}\"}) by (app) * 100", "legendFormat": "{{app}}" }],
          "fieldConfig": { "defaults": { "unit": "percent", "max": 100 } } },
        { "id": 2, "title": "System CPU Usage %", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 },
          "targets": [{ "expr": "avg(system_cpu_usage{namespace=\"retail-app\", app=~\"${service}\"}) by (app) * 100", "legendFormat": "{{app}}" }],
          "fieldConfig": { "defaults": { "unit": "percent", "max": 100 } } },
        { "id": 3, "title": "System Load Average (1m)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 },
          "targets": [{ "expr": "avg(system_load_average_1m{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 4, "title": "Open File Descriptors vs Max", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 },
          "description": "If open approaches max, the JVM will crash with 'Too many open files'.",
          "targets": [
            { "expr": "sum(process_files_open_files{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "open {{app}}" },
            { "expr": "max(process_files_max_files{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "max {{app}}" }
          ] },
        { "id": 5, "title": "Disk Free (GB)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 },
          "targets": [{ "expr": "min(disk_free_bytes{namespace=\"retail-app\", app=~\"${service}\"}) by (app) / 1073741824", "legendFormat": "{{app}}" }] },
        { "id": 6, "title": "Process Uptime (hours)", "type": "stat",
          "gridPos": { "h": 4, "w": 12, "x": 12, "y": 16 },
          "targets": [{ "expr": "max(process_uptime_seconds{namespace=\"retail-app\", app=~\"${service}\"}) by (app) / 3600", "legendFormat": "{{app}}" }] },
        { "id": 7, "title": "Thread Pool - Active vs Queue Remaining", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 24 },
          "targets": [
            { "expr": "sum(executor_active_threads{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "active {{app}}" },
            { "expr": "sum(executor_queued_tasks{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "queued {{app}}" }
          ] },
        { "id": 8, "title": "Thread Pool - Pool Size vs Max", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 24 },
          "targets": [
            { "expr": "sum(executor_pool_size_threads{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "current {{app}}" },
            { "expr": "max(executor_pool_max_threads{namespace=\"retail-app\", app=~\"${service}\"}) by (app)", "legendFormat": "max {{app}}" }
          ] }
      ]
    }
  istio-mesh.json: |
    {
      "id": null, "uid": "retail-istio",
      "title": "Retail Platform - Istio Service Mesh",
      "tags": ["retail", "istio", "mesh"], "timezone": "browser",
      "refresh": "10s", "time": { "from": "now-1h", "to": "now" },
      "templating": { "list": [{ "name": "workload", "label": "Destination Workload", "type": "custom",
        "query": "All : .+,user-service : user-service,order-service : order-service,payment-service : payment-service,inventory-service : inventory-service,frontend : frontend",
        "current": { "text": "All", "value": ".+" },
        "options": [
          { "text": "All", "value": ".+", "selected": true },
          { "text": "user-service", "value": "user-service", "selected": false },
          { "text": "order-service", "value": "order-service", "selected": false },
          { "text": "payment-service", "value": "payment-service", "selected": false },
          { "text": "inventory-service", "value": "inventory-service", "selected": false },
          { "text": "frontend", "value": "frontend", "selected": false }
        ] }] },
      "panels": [
        { "id": 1, "title": "Mesh Request Rate (by source → destination)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 },
          "targets": [{ "expr": "sum(rate(istio_requests_total{reporter=\"source\", source_workload_namespace=\"retail-app\", destination_workload=~\"${workload}\"}[5m])) by (source_workload, destination_workload)", "legendFormat": "{{source_workload}} → {{destination_workload}}" }] },
        { "id": 2, "title": "Mesh Error Rate (5xx)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 },
          "targets": [{ "expr": "sum(rate(istio_requests_total{reporter=\"source\", source_workload_namespace=\"retail-app\", destination_workload=~\"${workload}\", response_code=~\"5..\"}[5m])) by (source_workload, destination_workload)", "legendFormat": "{{source_workload}} → {{destination_workload}}" }] },
        { "id": 3, "title": "Mesh P99 Latency (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 },
          "targets": [{ "expr": "histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{reporter=\"source\", source_workload_namespace=\"retail-app\", destination_workload=~\"${workload}\"}[5m])) by (le, destination_workload))", "legendFormat": "{{destination_workload}}" }] },
        { "id": 4, "title": "Mesh Response Size (bytes/s)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 },
          "targets": [{ "expr": "sum(rate(istio_response_bytes_sum{reporter=\"source\", source_workload_namespace=\"retail-app\", destination_workload=~\"${workload}\"}[5m])) by (destination_workload)", "legendFormat": "{{destination_workload}}" }] },
        { "id": 5, "title": "TCP Connections Opened (rate)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 },
          "targets": [{ "expr": "sum(rate(istio_tcp_connections_opened_total{source_workload_namespace=\"retail-app\", source_workload=~\"${workload}\"}[5m])) by (source_workload, destination_workload)", "legendFormat": "{{source_workload}} → {{destination_workload}}" }] },
        { "id": 6, "title": "TCP Connections Closed (rate)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 16 },
          "targets": [{ "expr": "sum(rate(istio_tcp_connections_closed_total{source_workload_namespace=\"retail-app\", source_workload=~\"${workload}\"}[5m])) by (source_workload, destination_workload)", "legendFormat": "{{source_workload}} → {{destination_workload}}" }] },
        { "id": 7, "title": "gRPC Latency (ms) - Order→Inventory", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 24 },
          "targets": [{ "expr": "sum(rate(grpc_server_seconds_sum{namespace=\"retail-app\", app=~\"${workload}\"}[5m])) by (app, rpc_method) / sum(rate(grpc_server_seconds_count{namespace=\"retail-app\", app=~\"${workload}\"}[5m])) by (app, rpc_method) * 1000", "legendFormat": "{{app}} {{rpc_method}}" }] },
        { "id": 8, "title": "Envoy Sidecar Memory (MB)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 24 },
          "targets": [{ "expr": "sum(envoy_server_memory_allocated{namespace=\"retail-app\", app=~\"${workload}\"}) by (app) / 1048576", "legendFormat": "{{app}}" }] }
      ]
    }
  business-kpis.json: |
    {
      "id": null, "uid": "retail-business",
      "title": "Retail Platform - Business KPIs",
      "tags": ["retail", "business", "kpi"], "timezone": "browser",
      "refresh": "10s", "time": { "from": "now-1h", "to": "now" },
      "panels": [
        { "id": 1, "title": "Orders Created", "type": "stat",
          "gridPos": { "h": 4, "w": 6, "x": 0, "y": 0 },
          "description": "Orders created within the selected time range. Uses increase() to survive pod restarts.",
          "targets": [{ "expr": "sum(increase(orders_total{namespace=\"retail-app\"}[$__range]))", "legendFormat": "" }],
          "fieldConfig": { "defaults": { "color": { "mode": "thresholds" }, "thresholds": { "steps": [{"color": "green", "value": null}] } } } },
        { "id": 2, "title": "Payments Successful", "type": "stat",
          "gridPos": { "h": 4, "w": 6, "x": 6, "y": 0 },
          "targets": [{ "expr": "sum(increase(payments_success_total{namespace=\"retail-app\"}[$__range]))", "legendFormat": "" }],
          "fieldConfig": { "defaults": { "color": { "mode": "thresholds" }, "thresholds": { "steps": [{"color": "green", "value": null}] } } } },
        { "id": 3, "title": "Payments Failed", "type": "stat",
          "gridPos": { "h": 4, "w": 6, "x": 12, "y": 0 },
          "targets": [{ "expr": "sum(increase(payments_failure_total{namespace=\"retail-app\"}[$__range]))", "legendFormat": "" }],
          "fieldConfig": { "defaults": { "color": { "mode": "thresholds" }, "thresholds": { "steps": [{"color": "green", "value": null}, {"color": "red", "value": 1}] } } } },
        { "id": 4, "title": "Saga Completed / Failed", "type": "stat",
          "gridPos": { "h": 4, "w": 6, "x": 18, "y": 0 },
          "targets": [
            { "expr": "sum(increase(saga_completed_total{namespace=\"retail-app\"}[$__range]))", "legendFormat": "completed" },
            { "expr": "sum(increase(saga_failed_total{namespace=\"retail-app\"}[$__range]))", "legendFormat": "failed" }
          ] },
        { "id": 5, "title": "Order Creation Rate (orders/min)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 4 },
          "targets": [{ "expr": "sum(rate(orders_total{namespace=\"retail-app\"}[5m])) * 60", "legendFormat": "orders/min" }] },
        { "id": 6, "title": "Payment Success Rate %", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 4 },
          "targets": [{ "expr": "sum(rate(payments_success_total{namespace=\"retail-app\"}[5m])) / (sum(rate(payments_success_total{namespace=\"retail-app\"}[5m])) + sum(rate(payments_failure_total{namespace=\"retail-app\"}[5m]))) * 100", "legendFormat": "success %" }],
          "fieldConfig": { "defaults": { "unit": "percent", "min": 0, "max": 100 } } },
        { "id": 7, "title": "Order Creation Duration (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 12 },
          "targets": [{ "expr": "sum(rate(orders_creation_duration_seconds_sum{namespace=\"retail-app\"}[5m])) / sum(rate(orders_creation_duration_seconds_count{namespace=\"retail-app\"}[5m])) * 1000", "legendFormat": "avg ms" }] },
        { "id": 8, "title": "Payment Processing Duration (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 12 },
          "targets": [{ "expr": "sum(rate(payments_processing_duration_seconds_sum{namespace=\"retail-app\"}[5m])) / sum(rate(payments_processing_duration_seconds_count{namespace=\"retail-app\"}[5m])) * 1000", "legendFormat": "avg ms" }] },
        { "id": 9, "title": "Inventory Reservations (success vs failed)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 20 },
          "targets": [
            { "expr": "sum(rate(inventory_reservations_success_total{namespace=\"retail-app\"}[5m]))", "legendFormat": "success" },
            { "expr": "sum(rate(inventory_reservations_failed_total{namespace=\"retail-app\"}[5m]))", "legendFormat": "failed (no stock)" }
          ] },
        { "id": 10, "title": "Revenue (within time range)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 20 },
          "description": "Revenue accumulated within the dashboard time window. Uses increase() to handle pod restarts.",
          "targets": [
            { "expr": "sum(increase(orders_revenue_total{namespace=\"retail-app\"}[5m]))", "legendFormat": "order revenue" },
            { "expr": "sum(increase(payments_revenue_total{namespace=\"retail-app\"}[5m]))", "legendFormat": "payment revenue" }
          ] },
        { "id": 11, "title": "Items per Order (avg)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 28 },
          "targets": [{ "expr": "sum(rate(orders_items_total{namespace=\"retail-app\"}[5m])) / sum(rate(orders_total{namespace=\"retail-app\"}[5m]))", "legendFormat": "avg items/order" }] },
        { "id": 12, "title": "Orders Cancelled (rate)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 28 },
          "targets": [{ "expr": "sum(rate(orders_cancelled_total{namespace=\"retail-app\"}[5m]))", "legendFormat": "cancellations/s" }] }
      ]
    }
  failed-events.json: |
    {
      "id": null, "uid": "retail-failed-events",
      "title": "Retail Platform - Failed Events & Logs",
      "tags": ["retail", "failed-events", "logs"], "timezone": "browser",
      "refresh": "30s", "time": { "from": "now-6h", "to": "now" },
      "templating": {
        "list": [
          {
            "name": "service",
            "label": "Service",
            "type": "custom",
            "query": "All : user-service|order-service|payment-service|inventory-service,user-service : user-service,order-service : order-service,payment-service : payment-service,inventory-service : inventory-service",
            "current": { "text": "All", "value": "user-service|order-service|payment-service|inventory-service" },
            "options": [
              { "text": "All", "value": "user-service|order-service|payment-service|inventory-service", "selected": true },
              { "text": "user-service", "value": "user-service", "selected": false },
              { "text": "order-service", "value": "order-service", "selected": false },
              { "text": "payment-service", "value": "payment-service", "selected": false },
              { "text": "inventory-service", "value": "inventory-service", "selected": false }
            ]
          }
        ]
      },
      "panels": [
        { "id": 1, "title": "All Application Logs", "type": "logs",
          "gridPos": { "h": 10, "w": 24, "x": 0, "y": 0 },
          "datasource": "Loki",
          "targets": [{ "expr": "{namespace=\"retail-app\", container=~\"${service}\"}", "legendFormat": "" }],
          "options": { "showTime": true, "showLabels": true, "showCommonLabels": false, "wrapLogMessage": true, "enableLogDetails": true } },
        { "id": 2, "title": "Log Volume by Service", "type": "timeseries",
          "gridPos": { "h": 8, "w": 24, "x": 0, "y": 10 },
          "datasource": "Loki",
          "targets": [{ "expr": "sum(count_over_time({namespace=\"retail-app\", container=~\"${service}\"} [5m])) by (container)", "legendFormat": "{{container}}" }] },
        { "id": 3, "title": "DLT / CRITICAL / failed_events Logs", "type": "logs",
          "gridPos": { "h": 10, "w": 24, "x": 0, "y": 18 },
          "datasource": "Loki",
          "targets": [{ "expr": "{namespace=\"retail-app\", container=~\"${service}\"} |~ \"DLT|CRITICAL|failed_events\"", "legendFormat": "" }],
          "description": "Empty = healthy. Shows DLT, CRITICAL, and failed_events log entries.",
          "options": { "showTime": true, "showLabels": true, "enableLogDetails": true } },
        { "id": 4, "title": "DLT Events (rate)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 28 },
          "targets": [{ "expr": "sum(increase(kafka_dlt_events_total{namespace=\"retail-app\"}[5m])) by (app)", "legendFormat": "{{app}}" }] },
        { "id": 5, "title": "DLT Messages per Service (log rate)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 28 },
          "datasource": "Loki",
          "targets": [{ "expr": "sum(count_over_time({namespace=\"retail-app\", container=~\"${service}\"} |= \"DLT\" [5m])) by (container)", "legendFormat": "{{container}}" }] }
      ]
    }
  postgresql-server.json: |
    {
      "id": null, "uid": "retail-postgres",
      "title": "Retail Platform - PostgreSQL Server",
      "tags": ["retail", "postgres", "database"], "timezone": "browser",
      "refresh": "15s", "time": { "from": "now-1h", "to": "now" },
      "templating": { "list": [{ "name": "database", "label": "Database", "type": "custom",
        "query": "All : .+,user_db : user_db,order_db : order_db,payment_db : payment_db,inventory_db : inventory_db,postgres : postgres",
        "current": { "text": "All", "value": ".+" },
        "options": [
          { "text": "All", "value": ".+", "selected": true },
          { "text": "user_db", "value": "user_db", "selected": false },
          { "text": "order_db", "value": "order_db", "selected": false },
          { "text": "payment_db", "value": "payment_db", "selected": false },
          { "text": "inventory_db", "value": "inventory_db", "selected": false },
          { "text": "postgres", "value": "postgres", "selected": false }
        ] }] },
      "panels": [
        { "id": 1, "title": "Transactions / sec (commits + rollbacks)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 },
          "description": "Total transaction throughput. High rollback rate = application errors or contention.",
          "targets": [
            { "expr": "sum(rate(pg_stat_database_xact_commit{datname=~\"${database}\"}[5m])) by (datname)", "legendFormat": "commits {{datname}}" },
            { "expr": "sum(rate(pg_stat_database_xact_rollback{datname=~\"${database}\"}[5m])) by (datname)", "legendFormat": "rollbacks {{datname}}" }
          ] },
        { "id": 2, "title": "Cache Hit Ratio %", "type": "stat",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 },
          "description": "Should be > 99% in production. Low ratio = working set exceeds shared_buffers, causing disk I/O.",
          "targets": [{ "expr": "sum(pg_stat_database_blks_hit{datname=~\"${database}\", datname!~\"template.*\"}) by (datname) / (sum(pg_stat_database_blks_hit{datname=~\"${database}\", datname!~\"template.*\"}) by (datname) + sum(pg_stat_database_blks_read{datname=~\"${database}\", datname!~\"template.*\"}) by (datname)) * 100", "legendFormat": "{{datname}}" }],
          "options": { "reduceOptions": { "calcs": ["lastNotNull"] }, "textMode": "value_and_name", "graphMode": "none", "orientation": "horizontal" },
          "fieldConfig": { "defaults": { "unit": "percent", "decimals": 2, "min": 0, "max": 100, "thresholds": { "steps": [{"color": "red", "value": null}, {"color": "yellow", "value": 95}, {"color": "green", "value": 99}] } } } },
        { "id": 3, "title": "Active Connections", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 },
          "description": "Number of backends currently connected. Compare against max_connections (default 100).",
          "targets": [
            { "expr": "sum(pg_stat_database_numbackends{datname=~\"${database}\"}) by (datname)", "legendFormat": "{{datname}}" },
            { "expr": "pg_settings_max_connections", "legendFormat": "max_connections" }
          ] },
        { "id": 4, "title": "Deadlocks (rate)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 },
          "description": "Any deadlocks indicate application-level locking issues. Should always be 0.",
          "targets": [{ "expr": "sum(rate(pg_stat_database_deadlocks{datname=~\"${database}\"}[5m])) by (datname)", "legendFormat": "{{datname}}" }] },
        { "id": 5, "title": "Temp Files Written (rate)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 },
          "description": "Queries spilling to disk. Increase work_mem if this is high.",
          "targets": [{ "expr": "sum(rate(pg_stat_database_temp_files{datname=~\"${database}\"}[5m])) by (datname)", "legendFormat": "{{datname}}" }] },
        { "id": 6, "title": "Temp Bytes Written (rate MB/s)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 16 },
          "targets": [{ "expr": "sum(rate(pg_stat_database_temp_bytes{datname=~\"${database}\"}[5m])) by (datname) / 1048576", "legendFormat": "{{datname}}" }] },
        { "id": 7, "title": "Tuple Operations (rate/s)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 24 },
          "description": "Row-level operations across all tables. Useful for spotting write-heavy workloads.",
          "targets": [
            { "expr": "sum(rate(pg_stat_database_tup_inserted{datname=~\"${database}\"}[5m])) by (datname)", "legendFormat": "inserts {{datname}}" },
            { "expr": "sum(rate(pg_stat_database_tup_updated{datname=~\"${database}\"}[5m])) by (datname)", "legendFormat": "updates {{datname}}" },
            { "expr": "sum(rate(pg_stat_database_tup_deleted{datname=~\"${database}\"}[5m])) by (datname)", "legendFormat": "deletes {{datname}}" },
            { "expr": "sum(rate(pg_stat_database_tup_fetched{datname=~\"${database}\"}[5m])) by (datname)", "legendFormat": "fetches {{datname}}" }
          ] },
        { "id": 8, "title": "Database Size (MB)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 24 },
          "targets": [{ "expr": "pg_database_size_bytes{datname=~\"${database}\"} / 1048576", "legendFormat": "{{datname}}" }] },
        { "id": 9, "title": "Blocks Read vs Hit (rate/s)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 32 },
          "description": "Disk reads vs buffer cache hits. High disk reads = cache miss, consider increasing shared_buffers.",
          "targets": [
            { "expr": "sum(rate(pg_stat_database_blks_read{datname=~\"${database}\"}[5m])) by (datname)", "legendFormat": "disk read {{datname}}" },
            { "expr": "sum(rate(pg_stat_database_blks_hit{datname=~\"${database}\"}[5m])) by (datname)", "legendFormat": "cache hit {{datname}}" }
          ] },
        { "id": 10, "title": "Conflicts (rate)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 32 },
          "description": "Queries cancelled due to recovery conflicts (relevant with streaming replication).",
          "targets": [{ "expr": "sum(rate(pg_stat_database_conflicts{datname=~\"${database}\"}[5m])) by (datname)", "legendFormat": "{{datname}}" }] },
        { "id": 11, "title": "Rows Dead vs Live (top tables)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 40 },
          "description": "Dead tuples accumulate between VACUUMs. High dead:live ratio = bloat, VACUUM not keeping up.",
          "targets": [
            { "expr": "topk(10, pg_stat_user_tables_n_dead_tup)", "legendFormat": "dead {{relname}}" },
            { "expr": "topk(10, pg_stat_user_tables_n_live_tup)", "legendFormat": "live {{relname}}" }
          ] },
        { "id": 12, "title": "PostgreSQL Up", "type": "stat",
          "gridPos": { "h": 4, "w": 6, "x": 12, "y": 40 },
          "targets": [{ "expr": "pg_up", "legendFormat": "" }],
          "fieldConfig": { "defaults": { "mappings": [{"type": "value", "options": {"1": {"text": "UP", "color": "green"}, "0": {"text": "DOWN", "color": "red"}}}] } } },
        { "id": 13, "title": "Exporter Scrape Duration (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 6, "x": 18, "y": 40 },
          "targets": [{ "expr": "pg_exporter_last_scrape_duration_seconds * 1000", "legendFormat": "scrape ms" }] }
      ]
    }
  k8s-cluster.json: |
    {
      "id": null, "uid": "retail-k8s-cluster",
      "title": "Retail Platform - Kubernetes Cluster",
      "tags": ["retail", "kubernetes", "cluster"], "timezone": "browser",
      "refresh": "15s", "time": { "from": "now-1h", "to": "now" },
      "templating": { "list": [{ "name": "namespace", "label": "Namespace", "type": "custom",
        "query": "All : .+,retail-app : retail-app,retail-data : retail-data,retail-observe : retail-observe,istio-system : istio-system",
        "current": { "text": "All", "value": ".+" },
        "options": [
          { "text": "All", "value": ".+", "selected": true },
          { "text": "retail-app", "value": "retail-app", "selected": false },
          { "text": "retail-data", "value": "retail-data", "selected": false },
          { "text": "retail-observe", "value": "retail-observe", "selected": false },
          { "text": "istio-system", "value": "istio-system", "selected": false }
        ] }] },
      "panels": [
        { "id": 1, "title": "Node Count", "type": "stat",
          "gridPos": { "h": 4, "w": 4, "x": 0, "y": 0 },
          "targets": [{ "expr": "count(kube_node_info)", "legendFormat": "" }],
          "fieldConfig": { "defaults": { "color": { "mode": "thresholds" }, "thresholds": { "steps": [{"color": "green", "value": null}] } } } },
        { "id": 2, "title": "Pods Running", "type": "stat",
          "gridPos": { "h": 4, "w": 4, "x": 4, "y": 0 },
          "targets": [{ "expr": "sum(kube_pod_status_phase{namespace=~\"${namespace}\", phase=\"Running\"})", "legendFormat": "" }],
          "fieldConfig": { "defaults": { "color": { "mode": "thresholds" }, "thresholds": { "steps": [{"color": "green", "value": null}] } } } },
        { "id": 3, "title": "Pods Pending", "type": "stat",
          "gridPos": { "h": 4, "w": 4, "x": 8, "y": 0 },
          "targets": [{ "expr": "sum(kube_pod_status_phase{namespace=~\"${namespace}\", phase=\"Pending\"}) or vector(0)", "legendFormat": "" }],
          "fieldConfig": { "defaults": { "color": { "mode": "thresholds" }, "thresholds": { "steps": [{"color": "green", "value": null}, {"color": "yellow", "value": 1}] } } } },
        { "id": 4, "title": "Pods Failed", "type": "stat",
          "gridPos": { "h": 4, "w": 4, "x": 12, "y": 0 },
          "targets": [{ "expr": "sum(kube_pod_status_phase{namespace=~\"${namespace}\", phase=\"Failed\"}) or vector(0)", "legendFormat": "" }],
          "fieldConfig": { "defaults": { "color": { "mode": "thresholds" }, "thresholds": { "steps": [{"color": "green", "value": null}, {"color": "red", "value": 1}] } } } },
        { "id": 5, "title": "Container Restarts (last 1h)", "type": "stat",
          "gridPos": { "h": 4, "w": 4, "x": 16, "y": 0 },
          "targets": [{ "expr": "sum(increase(kube_pod_container_status_restarts_total{namespace=~\"${namespace}\"}[1h]))", "legendFormat": "" }],
          "fieldConfig": { "defaults": { "color": { "mode": "thresholds" }, "thresholds": { "steps": [{"color": "green", "value": null}, {"color": "yellow", "value": 5}, {"color": "red", "value": 20}] } } } },
        { "id": 6, "title": "Deployments Not Ready", "type": "stat",
          "gridPos": { "h": 4, "w": 4, "x": 20, "y": 0 },
          "targets": [{ "expr": "sum(kube_deployment_status_replicas{namespace=~\"${namespace}\"} - kube_deployment_status_replicas_available{namespace=~\"${namespace}\"}) or vector(0)", "legendFormat": "" }],
          "fieldConfig": { "defaults": { "color": { "mode": "thresholds" }, "thresholds": { "steps": [{"color": "green", "value": null}, {"color": "red", "value": 1}] } } } },
        { "id": 7, "title": "Node CPU Usage %", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 4 },
          "targets": [{ "expr": "100 - (avg by (node) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)", "legendFormat": "{{node}}" }],
          "fieldConfig": { "defaults": { "unit": "percent", "max": 100 } } },
        { "id": 8, "title": "Node Memory Usage %", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 4 },
          "targets": [{ "expr": "(1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100", "legendFormat": "{{instance}}" }],
          "fieldConfig": { "defaults": { "unit": "percent", "max": 100 } } },
        { "id": 9, "title": "Pod CPU Requests vs Limits (by namespace)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 12 },
          "targets": [
            { "expr": "sum(kube_pod_container_resource_requests{namespace=~\"${namespace}\", resource=\"cpu\"}) by (namespace)", "legendFormat": "requests {{namespace}}" },
            { "expr": "sum(kube_pod_container_resource_limits{namespace=~\"${namespace}\", resource=\"cpu\"}) by (namespace)", "legendFormat": "limits {{namespace}}" }
          ] },
        { "id": 10, "title": "Pod Memory Requests vs Limits (by namespace, MB)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 12 },
          "targets": [
            { "expr": "sum(kube_pod_container_resource_requests{namespace=~\"${namespace}\", resource=\"memory\"}) by (namespace) / 1048576", "legendFormat": "requests {{namespace}}" },
            { "expr": "sum(kube_pod_container_resource_limits{namespace=~\"${namespace}\", resource=\"memory\"}) by (namespace) / 1048576", "legendFormat": "limits {{namespace}}" }
          ] },
        { "id": 11, "title": "Deployment Replicas (desired vs available)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 20 },
          "targets": [
            { "expr": "kube_deployment_spec_replicas{namespace=~\"${namespace}\"}", "legendFormat": "desired {{deployment}}" },
            { "expr": "kube_deployment_status_replicas_available{namespace=~\"${namespace}\"}", "legendFormat": "available {{deployment}}" }
          ] },
        { "id": 12, "title": "Container Restarts (by pod)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 20 },
          "targets": [{ "expr": "increase(kube_pod_container_status_restarts_total{namespace=~\"${namespace}\"}[5m])", "legendFormat": "{{namespace}}/{{pod}}" }] },
        { "id": 13, "title": "Node Disk Usage %", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 28 },
          "targets": [{ "expr": "(1 - node_filesystem_avail_bytes{fstype=\"ext4\"} / node_filesystem_size_bytes{fstype=\"ext4\"}) * 100", "legendFormat": "{{instance}} {{mountpoint}}" }],
          "fieldConfig": { "defaults": { "unit": "percent", "max": 100 } } },
        { "id": 14, "title": "Node Network I/O (bytes/s)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 28 },
          "targets": [
            { "expr": "sum(rate(node_network_receive_bytes_total{device!~\"lo|veth.*|cali.*|flannel.*\"}[5m])) by (instance)", "legendFormat": "rx {{instance}}" },
            { "expr": "sum(rate(node_network_transmit_bytes_total{device!~\"lo|veth.*|cali.*|flannel.*\"}[5m])) by (instance)", "legendFormat": "tx {{instance}}" }
          ] },
        { "id": 15, "title": "HPA Current vs Desired Replicas", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 36 },
          "targets": [
            { "expr": "kube_horizontalpodautoscaler_status_current_replicas{namespace=~\"${namespace}\"}", "legendFormat": "current {{horizontalpodautoscaler}}" },
            { "expr": "kube_horizontalpodautoscaler_status_desired_replicas{namespace=~\"${namespace}\"}", "legendFormat": "desired {{horizontalpodautoscaler}}" }
          ] },
        { "id": 16, "title": "PVC Usage (bound vs pending)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 36 },
          "targets": [
            { "expr": "sum(kube_persistentvolumeclaim_status_phase{namespace=~\"${namespace}\", phase=\"Bound\"}) by (namespace)", "legendFormat": "bound {{namespace}}" },
            { "expr": "sum(kube_persistentvolumeclaim_status_phase{namespace=~\"${namespace}\", phase=\"Pending\"}) by (namespace)", "legendFormat": "pending {{namespace}}" }
          ] },
        { "id": 17, "title": "OOM Killed Containers", "type": "stat",
          "description": "Containers whose last termination was OOMKilled. Value of 1 means the container was OOM-killed on its most recent restart.",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 44 },
          "fieldConfig": { "defaults": { "color": { "mode": "fixed", "fixedColor": "red" }, "mappings": [{ "type": "value", "options": { "0": { "text": "OK", "color": "green" }, "1": { "text": "OOMKilled", "color": "red" } } }] } },
          "options": { "reduceOptions": { "calcs": ["lastNotNull"] }, "textMode": "auto", "colorMode": "background", "graphMode": "none" },
          "targets": [
            { "expr": "kube_pod_container_status_last_terminated_reason{namespace=~\"${namespace}\", reason=\"OOMKilled\"}", "legendFormat": "{{pod}} / {{container}}", "instant": true }
          ] },
        { "id": 18, "title": "Pod Memory Usage vs Limit (MB)", "type": "timeseries",
          "description": "Actual memory consumption vs the configured limit. Pods approaching the limit line are at risk of OOMKill.",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 44 },
          "fieldConfig": { "defaults": { "unit": "decmbytes" } },
          "targets": [
            { "expr": "sum(container_memory_working_set_bytes{namespace=~\"${namespace}\", container!=\"\", container!=\"istio-proxy\", container!=\"vault-agent\"}) by (pod) / 1024 / 1024", "legendFormat": "usage {{pod}}" },
            { "expr": "sum(kube_pod_container_resource_limits{namespace=~\"${namespace}\", resource=\"memory\", container!=\"istio-proxy\", container!=\"vault-agent\"}) by (pod) / 1024 / 1024", "legendFormat": "limit {{pod}}" }
          ] },
        { "id": 19, "title": "Container Restart Reasons", "type": "table",
          "description": "Shows the last termination reason for each container — OOMKilled, Error, etc.",
          "gridPos": { "h": 8, "w": 24, "x": 0, "y": 52 },
          "fieldConfig": { "defaults": { "custom": { "filterable": true } } },
          "options": { "sortBy": [{ "displayName": "Value", "desc": true }] },
          "targets": [
            { "expr": "kube_pod_container_status_last_terminated_reason{namespace=~\"${namespace}\"}", "legendFormat": "{{pod}} / {{container}} — {{reason}}", "format": "table", "instant": true }
          ],
          "transformations": [
            { "id": "organize", "options": { "excludeByName": { "Time": true, "__name__": true, "endpoint": true, "instance": true, "job": true, "service": true, "uid": true }, "renameByName": { "namespace": "Namespace", "pod": "Pod", "container": "Container", "reason": "Reason", "Value": "Count" } } }
          ] }
      ]
    }
  frontend-performance.json: |
    {
      "id": null, "uid": "retail-frontend",
      "title": "Retail Platform - Frontend Performance",
      "tags": ["retail", "frontend", "nextjs"], "timezone": "browser",
      "refresh": "10s", "time": { "from": "now-1h", "to": "now" },
      "panels": [
        { "id": 1, "title": "Frontend Request Rate (via Istio)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 },
          "targets": [{ "expr": "sum(rate(istio_requests_total{reporter=\"destination\", destination_workload=\"frontend\", destination_workload_namespace=\"retail-app\"}[5m])) by (response_code)", "legendFormat": "{{response_code}}" }] },
        { "id": 2, "title": "Frontend Error Rate (5xx)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 0 },
          "targets": [{ "expr": "sum(rate(istio_requests_total{reporter=\"destination\", destination_workload=\"frontend\", destination_workload_namespace=\"retail-app\", response_code=~\"5..\"}[5m]))", "legendFormat": "5xx errors/s" }] },
        { "id": 3, "title": "Frontend P50 / P95 / P99 Latency (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 },
          "targets": [
            { "expr": "histogram_quantile(0.50, sum(rate(istio_request_duration_milliseconds_bucket{reporter=\"destination\", destination_workload=\"frontend\"}[5m])) by (le))", "legendFormat": "p50" },
            { "expr": "histogram_quantile(0.95, sum(rate(istio_request_duration_milliseconds_bucket{reporter=\"destination\", destination_workload=\"frontend\"}[5m])) by (le))", "legendFormat": "p95" },
            { "expr": "histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket{reporter=\"destination\", destination_workload=\"frontend\"}[5m])) by (le))", "legendFormat": "p99" }
          ] },
        { "id": 4, "title": "Frontend Response Size (bytes/s)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 },
          "targets": [{ "expr": "sum(rate(istio_response_bytes_sum{reporter=\"destination\", destination_workload=\"frontend\"}[5m]))", "legendFormat": "bytes/s" }] },
        { "id": 5, "title": "Frontend → Backend Request Rate", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 },
          "targets": [{ "expr": "sum(rate(istio_requests_total{reporter=\"source\", source_workload=\"frontend\", source_workload_namespace=\"retail-app\"}[5m])) by (destination_workload)", "legendFormat": "→ {{destination_workload}}" }] },
        { "id": 6, "title": "Frontend → Backend Latency (ms)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 16 },
          "targets": [{ "expr": "histogram_quantile(0.95, sum(rate(istio_request_duration_milliseconds_bucket{reporter=\"source\", source_workload=\"frontend\"}[5m])) by (le, destination_workload))", "legendFormat": "p95 → {{destination_workload}}" }] },
        { "id": 7, "title": "Frontend Envoy Sidecar Memory (MB)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 24 },
          "targets": [{ "expr": "sum(envoy_server_memory_allocated{app=\"frontend\"}) by (pod) / 1048576", "legendFormat": "{{pod}}" }] },
        { "id": 8, "title": "Frontend Active Connections", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 24 },
          "targets": [
            { "expr": "sum(envoy_server_total_connections{app=\"frontend\"}) by (pod)", "legendFormat": "total {{pod}}" },
            { "expr": "sum(envoy_cluster_upstream_cx_active{app=\"frontend\"}) by (pod)", "legendFormat": "upstream {{pod}}" }
          ] },
        { "id": 9, "title": "Frontend Logs", "type": "logs",
          "gridPos": { "h": 8, "w": 24, "x": 0, "y": 32 },
          "datasource": "Loki",
          "targets": [{ "expr": "{namespace=\"retail-app\", container=\"frontend\"}", "legendFormat": "" }],
          "options": { "showTime": true, "showLabels": true, "wrapLogMessage": true, "enableLogDetails": true } }
      ]
    }

  dragonfly.json: |
    {
      "id": null, "uid": "retail-dragonfly",
      "title": "Retail Platform - Dragonfly Cache",
      "tags": ["retail", "dragonfly", "cache"], "timezone": "browser",
      "refresh": "10s", "time": { "from": "now-1h", "to": "now" },
      "panels": [
        { "id": 1, "title": "Memory Usage (MB)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 0 },
          "targets": [
            { "expr": "dragonfly_used_memory_bytes{job=\"dragonfly\"} / 1048576", "legendFormat": "used" },
            { "expr": "dragonfly_maxmemory_bytes{job=\"dragonfly\"} / 1048576", "legendFormat": "max" }
          ] },
        { "id": 2, "title": "Connected Clients", "type": "stat",
          "gridPos": { "h": 8, "w": 6, "x": 12, "y": 0 },
          "targets": [{ "expr": "dragonfly_connected_clients{job=\"dragonfly\"}", "legendFormat": "clients" }] },
        { "id": 3, "title": "Total Keys", "type": "stat",
          "gridPos": { "h": 8, "w": 6, "x": 18, "y": 0 },
          "targets": [{ "expr": "dragonfly_db_keys{job=\"dragonfly\"}", "legendFormat": "db{{db}}" }] },
        { "id": 4, "title": "Commands/sec", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 8 },
          "targets": [{ "expr": "rate(dragonfly_commands_processed_total{job=\"dragonfly\"}[1m])", "legendFormat": "cmd/s" }] },
        { "id": 5, "title": "Cache Hit Rate (%)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 8 },
          "targets": [{ "expr": "rate(dragonfly_keyspace_hits_total{job=\"dragonfly\"}[5m]) / (rate(dragonfly_keyspace_hits_total{job=\"dragonfly\"}[5m]) + rate(dragonfly_keyspace_misses_total{job=\"dragonfly\"}[5m])) * 100", "legendFormat": "hit rate" }] },
        { "id": 6, "title": "Network I/O (KB/s)", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 0, "y": 16 },
          "targets": [
            { "expr": "rate(dragonfly_net_input_bytes_total{job=\"dragonfly\"}[1m]) / 1024", "legendFormat": "in" },
            { "expr": "rate(dragonfly_net_output_bytes_total{job=\"dragonfly\"}[1m]) / 1024", "legendFormat": "out" }
          ] },
        { "id": 7, "title": "Evicted Keys/sec", "type": "timeseries",
          "gridPos": { "h": 8, "w": 12, "x": 12, "y": 16 },
          "targets": [{ "expr": "rate(dragonfly_evicted_keys_total{job=\"dragonfly\"}[1m])", "legendFormat": "evictions/s" }] }
      ]
    }

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: retail-observe
  labels:
    app: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      serviceAccountName: grafana

      # --- Init Container: Vault Agent ---
      # Fetches alerting credentials from Vault and writes them to
      # /vault/secrets/ (in-memory shared volume). Same pattern as
      # the backend services (user-service, order-service, etc.).
      initContainers:
        - name: vault-agent
          image: hashicorp/vault:1.17
          command: ["sh", "-c"]
          args:
            - |
              export VAULT_ADDR=http://vault.retail-data.svc.cluster.local:8200

              VAULT_TOKEN=$(vault write -field=token auth/kubernetes/login \
                role=grafana \
                jwt=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token))

              export VAULT_TOKEN

              vault kv get -field=teams-webhook-url secret/grafana/alerting > /vault/secrets/teams-webhook-url
              vault kv get -field=smtp-host secret/grafana/alerting > /vault/secrets/smtp-host
              vault kv get -field=smtp-user secret/grafana/alerting > /vault/secrets/smtp-user
              vault kv get -field=smtp-password secret/grafana/alerting > /vault/secrets/smtp-password
              vault kv get -field=smtp-from-address secret/grafana/alerting > /vault/secrets/smtp-from-address
              vault kv get -field=alert-email-to secret/grafana/alerting > /vault/secrets/alert-email-to

              echo "Vault secrets fetched successfully."

          volumeMounts:
            - name: vault-secrets
              mountPath: /vault/secrets

      containers:
        - name: grafana
          image: grafana/grafana:10.4.1
          # Shell wrapper: read Vault secret files into env vars, then exec Grafana.
          # Grafana reads SMTP config from env vars directly, so we source them
          # from the files written by the Vault Agent init container.
          command: ["sh", "-c"]
          args:
            - |
              export GF_SMTP_HOST=$(cat /vault/secrets/smtp-host)
              export GF_SMTP_USER=$(cat /vault/secrets/smtp-user)
              export GF_SMTP_PASSWORD=$(cat /vault/secrets/smtp-password)
              export GF_SMTP_FROM_ADDRESS=$(cat /vault/secrets/smtp-from-address)
              export TEAMS_WEBHOOK_URL=$(cat /vault/secrets/teams-webhook-url)
              export ALERT_EMAIL_TO=$(cat /vault/secrets/alert-email-to)
              exec /run.sh
          ports:
            - containerPort: 3000
              name: http
          env:
            - name: GF_SECURITY_ADMIN_USER
              value: "admin"
            - name: GF_SECURITY_ADMIN_PASSWORD
              value: "admin"
            - name: GF_USERS_ALLOW_SIGN_UP
              value: "false"
            - name: GF_UNIFIED_ALERTING_ENABLED
              value: "true"
            - name: GF_ALERTING_ENABLED
              value: "false"
            - name: GF_SMTP_ENABLED
              value: "true"
            - name: GF_SMTP_FROM_NAME
              value: "Retail Platform Alerts"
            - name: GF_SMTP_STARTTLS_POLICY
              value: "MandatoryStartTLS"
          volumeMounts:
            - name: vault-secrets
              mountPath: /vault/secrets
              readOnly: true
            - name: datasources
              mountPath: /etc/grafana/provisioning/datasources
            - name: dashboard-provider
              mountPath: /etc/grafana/provisioning/dashboards
            - name: dashboards
              mountPath: /var/lib/grafana/dashboards
            - name: alerting-contactpoints
              mountPath: /etc/grafana/provisioning/alerting/contactpoints.yaml
              subPath: contactpoints.yaml
            - name: alerting-mutetimings
              mountPath: /etc/grafana/provisioning/alerting/mutetimings.yaml
              subPath: mutetimings.yaml
            - name: alerting-policies
              mountPath: /etc/grafana/provisioning/alerting/policies.yaml
              subPath: policies.yaml
            - name: alerting-rules
              mountPath: /etc/grafana/provisioning/alerting/rules.yaml
              subPath: rules.yaml
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 500m
              memory: 512Mi
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 10
            periodSeconds: 10
      volumes:
        - name: vault-secrets
          emptyDir:
            medium: Memory
        - name: datasources
          configMap:
            name: grafana-datasources
        - name: dashboard-provider
          configMap:
            name: grafana-dashboard-provider
        - name: dashboards
          configMap:
            name: grafana-dashboards
        - name: alerting-contactpoints
          configMap:
            name: grafana-alerting-contactpoints
        - name: alerting-mutetimings
          configMap:
            name: grafana-alerting-mutetimings
        - name: alerting-policies
          configMap:
            name: grafana-alerting-policies
        - name: alerting-rules
          configMap:
            name: grafana-alerting-rules

---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: retail-observe
  labels:
    app: grafana
spec:
  type: NodePort
  selector:
    app: grafana
  ports:
    - port: 3000
      targetPort: 3000
      nodePort: 30030
      name: http
